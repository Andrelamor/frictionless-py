# -*- coding: utf-8 -*-
from __future__ import division
from __future__ import print_function
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest
from copy import deepcopy
from goodtables import validate
from goodtables.presets.datapackage import datapackage

#  multipart:
#  source: data/datapackages/multipart/datapackage.json
#  preset: datapackage
#  report:
#  - [1, 5, 1, 'type-or-format-error']

#  datapackages:
#  source:
#  - source: data/datapackages/valid/datapackage.json
#  preset: datapackage
#  - source: data/datapackages/invalid/datapackage.json
#  preset: datapackage
#  preset: nested
#  report:
#  - [3, 3, null, 'blank-row']
#  - [4, 4, null, 'blank-row']


# Validate


def test_source_pathlib_path_datapackage():
    report = validate(pathlib.Path('data/datapackages/valid/datapackage.json'))
    assert report['valid']


@pytest.mark.parametrize(
    'dp_path',
    ['data/datapackages/valid/datapackage.json', 'data/datapackages/valid.zip',],
)
def test_validate_datapackage_valid(log, dp_path):
    report = validate(dp_path)
    assert log(report) == []


@pytest.mark.parametrize(
    'dp_path',
    ['data/datapackages/invalid/datapackage.json', 'data/datapackages/invalid.zip',],
)
def test_validate_datapackage_invalid(log, dp_path):
    report = validate(dp_path)
    assert log(report) == [
        (1, 3, None, 'blank-row'),
        (2, 4, None, 'blank-row'),
    ]


# Validate (integrity)

DESCRIPTOR = {
    'resources': [
        {
            'name': 'resource1',
            'path': 'data/valid.csv',
            'bytes': 30,
            'hash': 'sha256:a1fd6c5ff3494f697874deeb07f69f8667e903dd94a7bc062dd57550cea26da8',
        }
    ]
}


def test_check_file_integrity(log):
    source = deepcopy(DESCRIPTOR)
    report = validate(source)
    assert log(report) == []


def test_check_file_integrity_invalid(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['bytes'] += 1
    source['resources'][0]['hash'] += 'a'
    report = validate(source)
    assert report['tables'][0]['errors'] == [
        {
            'code': 'source-error',
            'message': 'Calculated size "30" and hash "a1fd6c5ff3494f697874deeb07f69f8667e903dd94a7bc062dd57550cea26da8" differ(s) from declared value(s)',
            'message-data': {},
        }
    ]


def test_check_file_integrity_size(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['hash'] = None
    report = validate(source)
    assert log(report) == []


def test_check_file_integrity_size_invalid(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['bytes'] += 1
    source['resources'][0]['hash'] = None
    report = validate(source)
    assert report['tables'][0]['errors'] == [
        {
            'code': 'source-error',
            'message': 'Calculated size "30" differ(s) from declared value(s)',
            'message-data': {},
        }
    ]


def test_check_file_integrity_hash(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['bytes'] = None
    report = validate(source)
    assert log(report) == []


def test_check_file_integrity_hash_invalid(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['bytes'] = None
    source['resources'][0]['hash'] += 'a'
    report = validate(source)
    assert report['tables'][0]['errors'] == [
        {
            'code': 'source-error',
            'message': 'Calculated hash "a1fd6c5ff3494f697874deeb07f69f8667e903dd94a7bc062dd57550cea26da8" differ(s) from declared value(s)',
            'message-data': {},
        }
    ]


def test_check_file_integrity_invalid(log):
    source = deepcopy(DESCRIPTOR)
    source['resources'][0]['hash'] = 'not-supported-hash'
    report = validate(source)
    assert report['warnings'] == [
        'Resource "resource1" does not use the SHA256 hash. The check will be skipped',
    ]


# Preset


def test_preset_datapackage():
    warnings, tables = datapackage('data/datapackages/valid/datapackage.json')
    assert len(warnings) == 0
    assert len(tables) == 2


# Issues


def test_preset_datapackage_non_tabular_datapackage_issue_170():
    warnings, tables = datapackage('data/non_tabular_datapackage.json')
    assert len(warnings) == 0
    assert len(tables) == 2


def test_preset_datapackage_mixed_datapackage_issue_170():
    warnings, tables = datapackage('data/mixed_datapackage.json')
    assert len(warnings) == 0
    assert len(tables) == 2


def test_preset_datapackage_invalid_json_issue_192():
    warnings, tables = datapackage('data/invalid_json.json')
    assert len(warnings) == 1
    assert len(tables) == 0
    assert 'Unable to parse JSON' in warnings[0]


# Warnings


def test_validate_warnings_no():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage')
    assert len(report['warnings']) == 0


def test_validate_warnings_bad_datapackage_json():
    source = 'data/invalid_json.json'
    report = validate(source, preset='datapackage')
    assert len(report['warnings']) == 1
    assert 'Unable to parse JSON' in report['warnings'][0]


def test_validate_warnings_table_limit():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage', table_limit=1)
    assert len(report['warnings']) == 1
    assert 'table(s) limit' in report['warnings'][0]


def test_validate_warnings_row_limit():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage', row_limit=1)
    assert len(report['warnings']) == 2
    assert 'row(s) limit' in report['warnings'][0]
    assert 'row(s) limit' in report['warnings'][1]


def test_validate_warnings_error_limit():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage', error_limit=1)
    assert len(report['warnings']) == 2
    assert 'error(s) limit' in report['warnings'][0]
    assert 'error(s) limit' in report['warnings'][1]


def test_validate_warnings_table_and_row_limit():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage', table_limit=1, row_limit=1)
    assert len(report['warnings']) == 2
    assert 'table(s) limit' in report['warnings'][0]
    assert 'row(s) limit' in report['warnings'][1]


def test_validate_warnings_table_and_error_limit():
    source = 'data/datapackages/invalid/datapackage.json'
    report = validate(source, preset='datapackage', table_limit=1, error_limit=1)
    assert len(report['warnings']) == 2
    assert 'table(s) limit' in report['warnings'][0]
    assert 'error(s) limit' in report['warnings'][1]


# Misc


def test_validate_infer_datapackage_path(log):
    report = validate('data/datapackages/invalid/datapackage.json')
    assert report['error-count'] == 2


def test_validate_infer_datapackage_dict(log):
    with open('data/datapackages/invalid/datapackage.json') as file:
        report = validate(json.load(file))
        assert report['error-count'] == 2


# Datapackage with css dialect header false


def test_validate_datapackage_dialect_header_false(log):
    descriptor = {
        'resources': [
            {
                'name': 'name',
                'data': [['John', '22'], ['Alex', '33'], ['Paul', '44'],],
                'schema': {
                    'fields': [{'name': 'name'}, {'name': 'age', 'type': 'integer'},]
                },
                'dialect': {'header': False,},
            }
        ]
    }
    report = validate(descriptor)
    assert log(report) == []


def test_validate_datapackage_with_schema_issue_348(log):
    DESCRIPTOR = {
        'resources': [
            {
                'name': 'people',
                'data': [
                    ['id', 'name', 'surname'],
                    ['p1', 'Tom', 'Hanks'],
                    ['p2', 'Meryl', 'Streep'],
                ],
                'schema': {
                    'fields': [
                        {'name': 'id', 'type': 'string'},
                        {'name': 'name', 'type': 'string'},
                        {'name': 'surname', 'type': 'string'},
                        {'name': 'dob', 'type': 'date'},
                    ]
                },
            }
        ]
    }
    report = validate(DESCRIPTOR, checks=['structure', 'schema'])
    assert log(report) == [
        (1, None, 4, 'missing-header'),
    ]


def test_validate_datapackage_with_schema_structure_only_issue_348(log):
    DESCRIPTOR = {
        'resources': [
            {
                'name': 'people',
                'data': [
                    ['id', 'name', 'surname'],
                    ['p1', 'Tom', 'Hanks'],
                    ['p2', 'Meryl', 'Streep'],
                ],
                'schema': {
                    'fields': [
                        {'name': 'id', 'type': 'string'},
                        {'name': 'name', 'type': 'string'},
                        {'name': 'surname', 'type': 'string'},
                        {'name': 'dob', 'type': 'date'},
                    ]
                },
            }
        ]
    }
    report = validate(DESCRIPTOR, checks=['structure'])
    assert report['valid']


def test_validate_number_test_issue_232(log):
    # We check here that it doesn't raise exceptions
    source = 'data/number_test/datapackage.json'
    report = validate(source)
    assert not report['valid']


def test_validate_geopoint_required_constraint_issue_231(log):
    report = validate('data/datapackages/geopoint/datapackage.json')
    assert report['valid']


# FK

FK_DESCRIPTOR = {
    'resources': [
        {
            'name': 'cities',
            'data': [
                ['id', 'name', 'next_id'],
                [1, 'london', 2],
                [2, 'paris', 3],
                [3, 'rome', 4],
                [4, 'rio', None],
            ],
            'schema': {
                'fields': [
                    {'name': 'id', 'type': 'integer'},
                    {'name': 'name', 'type': 'string'},
                    {'name': 'next_id', 'type': 'integer'},
                ],
                'foreignKeys': [
                    {'fields': 'next_id', 'reference': {'resource': '', 'fields': 'id'},},
                    {
                        'fields': 'id',
                        'reference': {'resource': 'people', 'fields': 'label'},
                    },
                ],
            },
        },
        {
            'name': 'people',
            'data': [['label', 'population'], [1, 8], [2, 2], [3, 3], [4, 6],],
        },
    ],
}


def test_foreign_key(log):
    descriptor = deepcopy(FK_DESCRIPTOR)
    report = validate(descriptor, checks=['foreign-key'])
    assert log(report) == []


def test_foreign_key_not_defined_foreign_keys(log):
    descriptor = deepcopy(FK_DESCRIPTOR)
    del descriptor['resources'][0]['schema']['foreignKeys']
    report = validate(descriptor, checks=['foreign-key'])
    assert log(report) == []


def test_foreign_key_source_is_not_datapackage(log):
    report = validate('data/valid.csv', checks=['foreign-key'])
    assert log(report) == []


def test_foreign_key_self_referenced_resource_violation(log):
    descriptor = deepcopy(FK_DESCRIPTOR)
    del descriptor['resources'][0]['data'][4]
    report = validate(descriptor, checks=['foreign-key'])
    assert log(report) == [
        (1, 4, 3, 'foreign-key'),
    ]


def test_foreign_key_internal_resource_violation(log):
    descriptor = deepcopy(FK_DESCRIPTOR)
    del descriptor['resources'][1]['data'][4]
    report = validate(descriptor, checks=['foreign-key'])
    assert log(report) == [
        (1, 5, 1, 'foreign-key'),
    ]


def test_foreign_key_internal_resource_violation_non_existent(log):
    descriptor = deepcopy(FK_DESCRIPTOR)
    del descriptor['resources'][1]
    report = validate(descriptor, checks=['foreign-key'])
    assert log(report) == [
        (1, 2, 1, 'foreign-key'),
        (1, 3, 1, 'foreign-key'),
        (1, 4, 1, 'foreign-key'),
        (1, 5, 1, 'foreign-key'),
    ]


def test_foreign_key_external_resource(log):
    descriptor = 'data/datapackages_linked/cities/datapackage.json'
    report = validate(descriptor, checks=['structure', 'schema', 'foreign-key'])
    assert log(report) == []


def test_foreign_key_external_resource_errors(log):
    descriptor = 'data/datapackages_linked_errors/cities/datapackage.json'
    report = validate(descriptor, checks=['structure', 'schema', 'foreign-key'])
    assert log(report) == [
        (1, 4, 1, 'foreign-key'),  # self-referenced
        (1, 4, 3, 'foreign-key'),  # external
    ]


def test_foreign_key_external_resource_remote_datapackage(log):
    descriptor = {
        'resources': [
            {
                'name': 'countries',
                'data': [
                    ['Country Code', 'Country Name'],
                    ['PRT', 'Portugal'],
                    ['DRL', 'Dreamland'],
                ],
                'schema': {
                    'fields': [
                        {'name': 'Country Code', 'type': 'string'},
                        {'name': 'Country Name', 'type': 'string'},
                    ],
                    'foreignKeys': [
                        {
                            'fields': 'Country Code',
                            'reference': {
                                'package': 'https://raw.githubusercontent.com/datasets/gdp/master/datapackage.json',
                                'resource': 'gdp',
                                'fields': 'Country Code',
                            },
                        },
                    ],
                },
            }
        ]
    }
    report = validate(descriptor, checks=['structure', 'schema', 'foreign-key'])
    assert log(report) == [
        (1, 3, 1, 'foreign-key'),
    ]
